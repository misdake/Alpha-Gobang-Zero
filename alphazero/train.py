# coding:utf-8
import json
import math
import os
import time
import traceback
import random

import torch
import torch.nn.functional as F
from torch import nn, optim
from torch.optim.lr_scheduler import MultiStepLR
from torch.utils.data import DataLoader

from .alpha_zero_mcts import AlphaZeroMCTS
from .bubble_board import BubbleBoard
from .policy_value_net import PolicyValueNet
from .self_play_dataset import SelfPlayData, SelfPlayDataSet

from enum import Enum


class ValueType(Enum):
    WinDrawLose = 1
    BubbleCount = 2
    Combined = 3


def exception_handler(train_func):
    """ å¼‚å¸¸å¤„ç†è£…é¥°å™¨ """

    def wrapper(train_pipe_line, *args, **kwargs):
        try:
            train_func(train_pipe_line)
        except BaseException as e:
            if not isinstance(e, KeyboardInterrupt):
                traceback.print_exc()

            t = time.strftime('%Y-%m-%d_%H-%M-%S',
                              time.localtime(time.time()))
            train_pipe_line.save_model(
                f'last_policy_value_net_{t}', 'train_losses', 'games')

    return wrapper


class PolicyValueLoss(nn.Module):
    """ æ ¹æ® self-play äº§ç”Ÿçš„ `z` å’Œ `Ï€` è®¡ç®—è¯¯å·® """

    def __init__(self):
        super().__init__()

    def forward(self, p_hat, pi, value, z):
        """ å‰é¦ˆ

        Parameters
        ----------
        p_hat: Tensor of shape (N, board_len^2)
            å¯¹æ•°åŠ¨ä½œæ¦‚ç‡å‘é‡

        pi: Tensor of shape (N, board_len^2)
            `mcts` äº§ç”Ÿçš„åŠ¨ä½œæ¦‚ç‡å‘é‡

        value: Tensor of shape (N, )
            å¯¹æ¯ä¸ªå±€é¢çš„ä¼°å€¼

        z: Tensor of shape (N, )
            æœ€ç»ˆçš„æ¸¸æˆç»“æœç›¸å¯¹æ¯ä¸€ä¸ªç©å®¶çš„å¥–èµ
        """
        value_loss = F.mse_loss(value, z)
        policy_loss = -torch.sum(pi * p_hat, dim=1).mean()
        loss = value_loss + policy_loss
        return loss


class TrainModel:
    """ è®­ç»ƒæ¨¡å‹ """

    def __init__(self, name, board_w: int, board_h: int, n_mcts_iters, n_feature_planes, value_type=ValueType.BubbleCount):
        self.name = name
        self.c_puct = 3
        self.batch_size = 1000
        self.n_self_plays = 10000
        self.n_mcts_iters = n_mcts_iters
        self.is_save_game = True
        self.start_train_size = 2000
        self.value_type = value_type

        self.device = torch.device('cpu')
        self.bubble_board = BubbleBoard(board_w, board_h, n_feature_planes)

        # åˆ›å»ºç­–ç•¥-ä»·å€¼ç½‘ç»œå’Œè’™ç‰¹å¡æ´›æœç´¢æ ‘
        self.policy_value_net = self.__get_policy_value_net(board_w, board_h)
        self.mcts = AlphaZeroMCTS(self.policy_value_net, c_puct=self.c_puct, n_iters=n_mcts_iters, is_self_play=True)

        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = optim.Adam(self.policy_value_net.parameters(), lr=0.01, weight_decay=1e-4)
        self.loss = PolicyValueLoss()
        self.lr_scheduler = MultiStepLR(self.optimizer, [100, 300, 500], gamma=0.1)

        # åˆ›å»ºæ•°æ®é›†
        self.dataset = SelfPlayDataSet(board_w, board_h)

        # è®°å½•æ•°æ®
        self.train_losses = self.__load_data(f'log/train_losses_{self.name}.json')
        self.games = self.__load_data('log/games.json')

    def __self_play(self, train_iter):
        """ è‡ªæˆ‘åšå¼ˆä¸€å±€

        Returns
        -------
        self_play_data: namedtuple
            è‡ªæˆ‘åšå¼ˆæ•°æ®ï¼Œæœ‰ä»¥ä¸‹ä¸‰ä¸ªæˆå‘˜:
            * `pi_list`: è’™ç‰¹å¡æ´›æ ‘æœç´¢äº§ç”Ÿçš„åŠ¨ä½œæ¦‚ç‡å‘é‡ Ï€ ç»„æˆçš„åˆ—è¡¨
            * `z_list`: ä¸€å±€ä¹‹ä¸­æ¯ä¸ªåŠ¨ä½œçš„ç©å®¶ç›¸å¯¹æœ€åçš„æ¸¸æˆç»“æœçš„å¥–èµåˆ—è¡¨
            * `feature_planes_list`: ä¸€å±€ä¹‹ä¸­æ¯ä¸ªåŠ¨ä½œå¯¹åº”çš„ç‰¹å¾å¹³é¢ç»„æˆçš„åˆ—è¡¨
        """
        # åˆå§‹åŒ–æ£‹ç›˜å’Œæ•°æ®å®¹å™¨
        self.policy_value_net.eval()
        board = self.bubble_board
        board.clear_board()

        # 50%æ¦‚ç‡éšæœºä¸€ä¸ªå¼€å±€ï¼Œé˜²æ­¢è‡ªå¯¹å¼ˆæƒ…å†µä¸‹å¼€å±€å½¢æˆå®šå¼
        if random.randint(0, 1) == 1:
            for _ in range(random.randint(0, 100)):
                choice = random.choice(board.available_actions)
                board.do_action(choice)

        pi_list, feature_planes_list, players = [], [], []
        action_list, z_list = [], []

        # å¼€å§‹ä¸€å±€æ¸¸æˆ
        while True:
            player = board.current_player
            curr_reward = board.get_state_reward(player)
            action, pi = self.mcts.get_action(board)

            if train_iter % 10 == 0:
                print()
                board.print((action % self.bubble_board.board_w, action // self.bubble_board.board_w))

            # ä¿å­˜æ¯ä¸€æ­¥çš„æ•°æ®
            feature_plane = board.get_feature_planes(player)
            feature_planes_list.append(feature_plane)
            players.append(player)
            action_list.append(action)
            pi_list.append(pi)
            board.do_action(action)

            # åˆ¤æ–­æ¸¸æˆæ˜¯å¦ç»“æŸ
            is_over, winner = board.is_game_over_with_limit()

            # è®°å½•çŠ¶æ€ä»·å€¼
            # next_reward = board.get_state_reward(player)
            # action_reward = next_reward - curr_reward

            value = math.tanh(curr_reward)
            z_list.append(value)

            if player > 0:
                print('+', end='')
            else:
                print('-', end='')
            print(f'{action:02}({value:.3}) ', end='')
            if board.action_len % 10 == 0:
                print(f'  --{board.action_len}')

            if is_over:
                print()

                if self.value_type == ValueType.Combined:
                    if winner != 0:
                        wdl = [1 if i == winner else -1 for i in players]
                    else:
                        wdl = [0] * len(players)
                    state_ratio = 0.98 ** train_iter  # 0.66@20, 0.36@50, 0.13@100
                    wdl_ratio = 1.0 - state_ratio
                    z_list = [state_ratio * z_list[i] + wdl_ratio * wdl[i] for i in range(len(players))]

                if self.value_type == ValueType.WinDrawLose:
                    # æ”¹ç”¨ä¼ ç»Ÿæ£‹ç±»çš„æ–¹æ³•è®¾ç½®value
                    if winner != 0:
                        z_list = [1 if i == winner else -1 for i in players]
                    else:
                        z_list = [0] * len(players)

                print(f'winner {winner}')

                break

        # é‡ç½®æ ¹èŠ‚ç‚¹
        self.mcts.reset_root()

        # è¿”å›æ•°æ®
        if self.is_save_game:
            self.games.append(action_list)

        self_play_data = SelfPlayData(pi_list=pi_list, z_list=z_list, feature_planes_list=feature_planes_list)
        return self_play_data

    @exception_handler
    def train(self):
        train_iter = 0
        """ è®­ç»ƒæ¨¡å‹ """
        for i in range(self.n_self_plays):
            print(f'ğŸ¹ æ­£åœ¨è¿›è¡Œç¬¬ {i + 1} å±€è‡ªæˆ‘åšå¼ˆæ¸¸æˆ...')
            self.dataset.append(self.__self_play(train_iter))

            print(f'æ•°æ®é›†å®¹é‡ {len(self.dataset)}')

            # å¦‚æœæ•°æ®é›†ä¸­çš„æ•°æ®é‡å¤§äº start_train_size å°±è¿›è¡Œä¸€æ¬¡è®­ç»ƒ
            if len(self.dataset) >= self.start_train_size:
                for _ in range(2):
                    data_loader = iter(DataLoader(self.dataset, self.batch_size, shuffle=True, drop_last=False))
                    print(f'ğŸ’Š ç¬¬ {train_iter + 1} æ¬¡è®­ç»ƒ...')
                    train_iter += 1

                    self.policy_value_net.train()
                    # éšæœºé€‰å‡ºä¸€æ‰¹æ•°æ®æ¥è®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
                    feature_planes, pi, z = next(data_loader)
                    feature_planes = feature_planes.to(self.device)
                    pi, z = pi.to(self.device), z.to(self.device)

                    # å‰é¦ˆ
                    p_hat, value = self.policy_value_net(feature_planes)
                    # æ¢¯åº¦æ¸…é›¶
                    self.optimizer.zero_grad()
                    # è®¡ç®—æŸå¤±
                    loss = self.loss(p_hat, pi, value.flatten(), z)
                    # è¯¯å·®åå‘ä¼ æ’­
                    loss.backward()
                    # æ›´æ–°å‚æ•°
                    self.optimizer.step()
                    # å­¦ä¹ ç‡é€€ç«
                    self.lr_scheduler.step()

                    # è®°å½•è¯¯å·®
                    self.train_losses.append([i, loss.item()])
                    print(f"ğŸš© train_loss = {loss.item():<10.5f}\n")

                    if train_iter % 50 == 0:
                        model_path = f'model/checkpoint/{self.name}_{train_iter}.pth'
                        torch.save(self.mcts.policy_value_net, model_path)

    def save_model(self, model_name: str, loss_name: str, game_name: str):
        """ ä¿å­˜æ¨¡å‹

        Parameters
        ----------
        model_name: str
            æ¨¡å‹æ–‡ä»¶åç§°ï¼Œä¸åŒ…å«åç¼€

        loss_name: str
            æŸå¤±æ–‡ä»¶åç§°ï¼Œä¸åŒ…å«åç¼€

        game_name: str
            è‡ªå¯¹å¼ˆæ£‹è°±åç§°ï¼Œä¸åŒ…å«åç¼€
        """
        os.makedirs('model', exist_ok=True)

        path = f'model/{model_name}.pth'
        self.policy_value_net.eval()
        torch.save(self.policy_value_net, path)
        print(f'ğŸ‰ å·²å°†å½“å‰æ¨¡å‹ä¿å­˜åˆ° {os.path.join(os.getcwd(), path)}')

        # ä¿å­˜æ•°æ®
        with open(f'log/{loss_name}.json', 'w', encoding='utf-8') as f:
            json.dump(self.train_losses, f)

        if self.is_save_game:
            with open(f'log/{game_name}.json', 'w', encoding='utf-8') as f:
                json.dump(self.games, f)

    def __do_mcts_action(self, mcts):
        """ è·å–åŠ¨ä½œ """
        action = mcts.get_action(self.bubble_board)
        self.bubble_board.do_action(action)
        is_over, winner = self.bubble_board.is_game_over()
        return is_over, winner

    def __get_policy_value_net(self, board_w: int, board_h: int):
        """ åˆ›å»ºç­–ç•¥-ä»·å€¼ç½‘ç»œï¼Œå¦‚æœå­˜åœ¨å†å²æœ€ä¼˜æ¨¡å‹åˆ™ç›´æ¥è½½å…¥æœ€ä¼˜æ¨¡å‹ """
        os.makedirs('checkpoint', exist_ok=True)
        os.makedirs('history', exist_ok=True)

        best_model = 'best_policy_value_net.pth'
        history_models = sorted([i for i in os.listdir('model') if i.startswith('last')])

        # ä»å†å²æ¨¡å‹ä¸­é€‰å–æœ€æ–°æ¨¡å‹
        model = history_models[-1] if history_models else best_model
        model = f'model/{model}'
        if os.path.exists(model):
            print(f'ğŸ’ è½½å…¥æ¨¡å‹ {model} ...\n')
            net = torch.load(model, map_location=torch.device('cpu')).to(self.device)  # type:PolicyValueNet
        else:
            print(f'ğŸ’ åˆå§‹åŒ–æ¨¡å‹ {model} ...\n')
            net = PolicyValueNet(n_feature_planes=self.bubble_board.n_feature_planes,
                                 board_w=board_w, board_h=board_h).to(self.device)

        return net

    def __load_data(self, path: str):
        """ è½½å…¥å†å²æŸå¤±æ•°æ® """
        data = []
        try:
            with open(path, encoding='utf-8') as f:
                data = json.load(f)
        except:
            os.makedirs('log', exist_ok=True)

        return data
